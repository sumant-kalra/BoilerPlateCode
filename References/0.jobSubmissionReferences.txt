1. SLURM Job Scheduler: https://slurm.schedmd.com/documentation.html
                        https://support.ceci-hpc.be/doc/_contents/QuickStart/SubmittingJobs/SlurmTutorial.html 
   Tutorial:            https://www.youtube.com/watch?v=hFsPY0Ti1gE&t=3211s&ab_channel=CECIandCISMHPC 

2. PBS Job Scheduler (Portable Batch System): https://2021.help.altair.com/2021.1.2/PBS%20Professional/PBSUserGuide2021.1.2.pdf  

----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
1. SLURM Job Scheduler:
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
1. SBATCH Parameters:
    --constraints                                   // choose a specific feature (e.g. a processor type or a network type)
    --gres(or --gpu)                                // use a specific resource (e.g. a GPU) 
    --license                                       // access a specific licensed software 
    --partition                                     // choose a partition
    --test-only                                     // Enquire when the job will start 

    --job-name=SomeName                             // Sets the job name 
    --comment="Something that I want"               // Attaches a comment to the job
            
    --mail-type=BEGIN|END|FAILED|ALL|TIME_LIMIT_90  // Refer to the documentation 
    --mail-user=my@mail.com 

    --output=resultFileName-%j.txt                  // %j is replaced with JobId
    --error=errorFileName-%j.txt
            
    --time=<duration>                               // Refer to the documentation for the duration format
    --exclusive                                     // Allocated nodes (cores of the nodes) are not shared with other jobs but the memory allocated is only as per request 
    --exclusive --mem=0                             // exclusive + all the memory on a node

    --gpus=N                                        // N GPUs for the job 
    --gpus=Teslav100:1                              // 1 specific GPU for the job
    --gres=gpu:N                                    // N GPUs per allocated node 
    --gres=gpu:Teslav100:1                          // 1 specific GPU per allocated Node

    USAGE:
    # Define the number of tasks, the number of cpus required for each task and memory required for each cpu
       --ntasks=N --cpus-per-task=K --mem-per-cpu=Qu                                 // Allocates N*K CPUs, N processes with K threads in each process, Qu for each core; 
                                                                                        Same or different nodes 
                                                                                        variable $SLURM_CPUS_PER_TASK is set with --cpus-per-task 
    # Control the number of tasks per node (one node can run multiple tasks, how many tasks per node?)
       --ntasks=N --cpus-per-task=K --mem-per-cpu=Qu --ntasks-per-node=M             // Allocates N*K CPUs, N processes with K threads in each process, Qu for each core;
                                                                                        N tasks distributed over different nodes with M tasks on each node;
                                                                                        If M==1, each task is run on a different node;
    NOTE: 
    - There is another option --nodes that can control the distribution of tasks on nodes when used with --ntasks but preferably use --tasks-per-node for better control 

2. Monitor and Inspect the submitted Jobs: squeue; sstat; scontrol; sacct 
    [PENDING OR RUNNING]
    (a) squeue : Information about the jobs in slurm scheduling queue [PENDING OR RUNNING] 
        squeue                                                                          // All the submitted jobs 
        squeue --me                                                                     // Submitted by me
        squeue --me --start                                                             // Also displays the expected start time of the jobs in pending status (PD) 
        squeue --partition=debug                                                        // Filter the jobs with partition=debug 
        squeue --Format=jobid,partition,timeused,timelimit --partition=debug            // Format the squeue output + filter partition=debug
    
    [RUNNING]
    (b) sstat : List status info for a currently running job [RUNNING]
        sstat --format=AveCPU,AvePages,AveRSS,AveVMSize,JobID -j <jobid> --allsteps     // Formatted status info for a job = jobId  

    [PENDING OR RUNNING OR COMPLETED]
    (c) scontrol show : Detailed information about the job [PENDING OR RUNNING OR COMPLETED] 
        scontrol show jobid -dd <jobId> 
    
    [RUNNING or COMPLETED]
    (d) sacct : Statistics about the job [RUNNING or COMPLETED]; Useful for completed job to obtain information like run-time, memory used etc that was not available during the run.
        sacct -j <jobid> --format=JobID,JobName,MaxRSS,Elapsed                          // Formatted statistics for a job = jobId
        sacct -u <username> --format=JobID,JobName,MaxRSS,Elapsed                       // Formatted statistics of all the jobs submitted by user = username 

    Note: Details on the completed jobs are available in slurm database only for a limited peroid of time.

3. Control your jobs: 
    scontrol show <jobId>                                                               // Show the details of the job with <jobId>
    scontrol update jobid=<jobId> <parameter>=<value>                                   // Modify a job in Pending state 
    scancel <jobId>                                                                     // cancel the job with <jobId> 
    scancel -n <jobName>                                                                // cancel the job with <jobName>

4. Discover the cluster features:
    sinfo 
    sinfo "%4D %9P %25f %.5c %.8m %G"
    sacctmgr list qos                                                                   // QOS: Quality of service: used by sysadmin to organize/prioritize jobs
    scontrol show licenses                                                              // License: used to organize software license distribution to jobs 























----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
SLURM Job Scheduler (Old Notes):
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
 
sbatch -A cldrn --reservation=cauldron -p cdis -w houcy1-n-cp101l20 -C rhel7 -n 2 --nodes=1 runMysql20.sh

sbatch -A cldrn --reservation=cauldron -p cdis -w houcy1-n-cp101l20 -C rhel7 -n 4 --nodes=1 runMigrationScript20.sh

sbatch --parsable -w amsdc2-n-cp011a38 -A cldrn -C rhel7 -n 2 --nodes=1 runMysql.sh
sbatch --parsable -w amsdc2-n-cp011a38 -A cldrn -C rhel7 -n 2 --nodes=1 runMigrationScript.sh

squeue -u <userID>
squeue --me
squeue -j <JobID>
scancel <JobID>

scontrol show jobid -dd <jobid>

sacct -X --format="JobID,JobName%60,nodelist%30" -u <userId> -s R
																-s : Status
																R : Run
																PD : Pending
squeue -o"%.7i %.9P %.8j %.8u %.2t %.10M %.6D %C"

sacct -X --format="nodelist%30" | grep 

sbatch -A cldrn --reservation=cauldron -p cdis -w houcy1-n-cp101l21 -C rhel7 -n 4 --nodes=1 runMigrationScriptTest.sh

=============================================================================
References

LSF to SLURM:
https://help.jasmin.ac.uk/article/4891-lsf-to-slurm-quick-reference
https://elwe.rhrk.uni-kl.de/elwetritsch/slurm_guide.shtml

https://slurm.schedmd.com/documentation.html

https://stackoverflow.com/questions/43767866/slurm-srun-vs-sbatch-and-their-parameters

https://www.rit.edu/researchcomputing/instructions/Slurm-Basic-Commands
https://hpc.llnl.gov/banks-jobs/running-jobs/slurm-commands



----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
2. PBS Job Scheduler: 
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
1. Submitting an executable as a job
    echo <executableFile> | qsub            # executableFile can be a shell command or any other application created by developer 
                                            # NOTE THAT MANY OTHER SCHEDULERS LIKE SLURM DONOT ALLOW TO SUBMIT AN EXECUTABLE FILE DIRECTLY TO THE SCHEDULER 
                                            # IN THOSE CASES, THE PROGRAM MUST BE CALLED FROM A BASH SCRIPT AND THE BATCH SCRIPT IN SUBMITTED TO THE SCHEDULER

## NOTE : MUST HAVE AN EMPTY LINE AT THE END OF THE BATCH SCRIPT OTHERWISE THE LAST COMMAND WILL NOT BE EXECUTED
2. Submit a Batch script / Submit script / Shell Script (Bash by default) (Preferred as it provides much more flexibility and the script can be used as metadata)
    qsub <script.sh>                        # script.sh is processed by shell (bash by default)
                                            # may contains PBS directives in lines beginning with #PBS

3. Querying Jobs
    qstat                                   
    qstat -f <JobId>                        # 567.c008; JobId=567, ServerName=c008 

4. Deleting Jobs  
    qdel <JobId>
    qdel all                                # Delete all the jobs run by you    

5. Sample PBS Script : https://github.com/sumant-kalra/HPC_Optimization/blob/main/Notes/pbs-QuickGuide.pdf (Refer to the doc for all the details) 

    #!/usr/bin/env bash 

    ### Job Name 
    #PBS -N hello_world_job

    ### Output files 
    #PBS -o hello_world_job.stdout
    #PBS -e hello_world_job.stderr 

    ### Changing Working Directory : If omitted then the home directory is used. Sets the environment variable PBS_O_INITDIR 
    #PBS -d ~/MyDir/Project 

    ### Comma separated list of desired resources : nodes; walltime; cput; mem; ncpus
    #PBS -l nodes=2,walltime=00:30:00,cput=00:10:00,ncpus=4,mem=4gb

    cat $PBS_NODEFILE


